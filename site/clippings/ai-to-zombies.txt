Rationality: From AI to Zombies (Eliezer Yudkowsky)
- Highlight Loc. 270-71  | Added on Wednesday, December 23, 2015, 06:26 PM

Walking through all of that, from a dozen different angles, can sometimes convey a glimpse of the central rhythm. 
==========
Rationality: From AI to Zombies (Eliezer Yudkowsky)
- Highlight Loc. 783  | Added on Wednesday, December 23, 2015, 07:00 PM

You should not ignore something just because you can’t define it. 
==========
Rationality: From AI to Zombies (Eliezer Yudkowsky)
- Highlight Loc. 878  | Added on Wednesday, December 23, 2015, 07:12 PM

Which is to say: Adding detail can make a scenario SOUND MORE PLAUSIBLE, even though 
==========
Rationality: From AI to Zombies (Eliezer Yudkowsky)
- Highlight Loc. 878-79  | Added on Wednesday, December 23, 2015, 07:12 PM

Which is to say: Adding detail can make a scenario SOUND MORE PLAUSIBLE, even though the event necessarily BECOMES LESS PROBABLE. 
==========
Rationality: From AI to Zombies (Eliezer Yudkowsky)
- Highlight Loc. 879-81  | Added on Wednesday, December 23, 2015, 07:12 PM

If so, then, hypothetically speaking, we might find futurists spinning unconscionably plausible and detailed future histories, or find people swallowing huge packages of unsupported claims bundled with a few strong-sounding assertions at the center. 
==========
Rationality: From AI to Zombies (Eliezer Yudkowsky)
- Highlight Loc. 894-96  | Added on Wednesday, December 23, 2015, 07:13 PM

They would need to notice the conjunction of two entire details, and be shocked by the audacity of anyone asking them to endorse such an insanely complicated prediction. And they would need to penalize the probability substantially—a factor of four, at least, according to the experimental details. 
==========
Rationality: From AI to Zombies (Eliezer Yudkowsky)
- Highlight Loc. 894-95  | Added on Wednesday, December 23, 2015, 07:14 PM

They would need to notice the conjunction of two entire details, and be shocked by the audacity of anyone asking them to endorse such an insanely complicated prediction. 
==========
Rationality: From AI to Zombies (Eliezer Yudkowsky)
- Highlight Loc. 923  | Added on Wednesday, December 23, 2015, 07:18 PM

You have to disentangle the details. You have to hold up every one independently, and ask, “How do we know this detail?” 
==========
Rationality: From AI to Zombies (Eliezer Yudkowsky)
- Highlight Loc. 923-25  | Added on Wednesday, December 23, 2015, 07:19 PM

Someone sketches out a picture of humanity’s descent into nanotechnological warfare, where China refuses to abide by an international control agreement, followed by an arms race . . . Wait a minute—how do you know it will be China? 
==========
Rationality: From AI to Zombies (Eliezer Yudkowsky)
- Highlight Loc. 957-58  | Added on Wednesday, December 23, 2015, 07:20 PM

More generally, this phenomenon is known as the “planning fallacy.” The planning fallacy is that people think they can plan, ha ha. 
==========
Rationality: From AI to Zombies (Eliezer Yudkowsky)
- Highlight Loc. 959-62  | Added on Wednesday, December 23, 2015, 07:20 PM

Asking subjects for their predictions based on realistic “best guess” scenarios; and Asking subjects for their hoped-for “best case” scenarios . . . . . . produced indistinguishable results. 
==========
Rationality: From AI to Zombies (Eliezer Yudkowsky)
- Highlight Loc. 964-65  | Added on Wednesday, December 23, 2015, 07:20 PM

Reality, it turns out, usually delivers results somewhat worse than the “worst case.” 
==========
Rationality: From AI to Zombies (Eliezer Yudkowsky)
- Highlight Loc. 1082-83  | Added on Wednesday, December 23, 2015, 07:29 PM

Conversely, if you say something blatantly obvious and the other person doesn’t see it, they’re the idiot, or they’re being deliberately obstinate to annoy you. 
==========
Rationality: From AI to Zombies (Eliezer Yudkowsky)
- Highlight Loc. 1099-1101  | Added on Wednesday, December 23, 2015, 07:30 PM

A clear argument has to lay out an inferential pathway, starting from what the audience already knows or accepts. If you don’t recurse far enough, you’re just talking to yourself. 
==========
Rationality: From AI to Zombies (Eliezer Yudkowsky)
- Highlight Loc. 1121-23  | Added on Wednesday, December 23, 2015, 07:32 PM

Mice can see, but they can’t understand seeing. You can understand seeing, and because of that, you can do things that mice cannot do. Take a moment to marvel at this, for it is indeed marvelous. 
==========
Rationality: From AI to Zombies (Eliezer Yudkowsky)
- Highlight Loc. 1187-89  | Added on Sunday, December 27, 2015, 05:26 AM

It is a great strength of Homo sapiens that we can, better than any other species in the world, learn to model the unseen. It is also one of our great weak points. Humans often believe in things that are not only unseen but unreal. 
==========
Rationality: From AI to Zombies (Eliezer Yudkowsky)
- Highlight Loc. 1204-6  | Added on Sunday, December 27, 2015, 05:30 AM

It is even better to ask: what experience must not happen to you? Do you believe that élan vital explains the mysterious aliveness of living beings? Then what does this belief not allow to happen—what would definitely falsify this belief? 
==========
Rationality: From AI to Zombies (Eliezer Yudkowsky)
- Highlight Loc. 1440-44  | Added on Sunday, December 27, 2015, 05:48 AM

As Robin Hanson describes it, the ability to have potentially divisive conversations is a limited resource . If you can think of ways to pull the rope sideways , you are justified in expending your limited resources on relatively less common issues where marginal discussion offers relatively higher marginal payoffs. 
==========
Rationality: From AI to Zombies (Eliezer Yudkowsky)
- Highlight Loc. 1598  | Added on Thursday, December 31, 2015, 08:26 PM

At the Singularity Summit 2007, one of the speakers called for democratic, multinational development of Artificial Intelligence. 
==========
Rationality: From AI to Zombies (Eliezer Yudkowsky)
- Highlight Loc. 1636-37  | Added on Thursday, December 31, 2015, 08:28 PM

I am tempted to give a talk sometime that consists of nothing but applause lights, and see how long it takes for the audience to start laughing: 
==========
Rationality: From AI to Zombies (Eliezer Yudkowsky)
- Highlight Loc. 1637-44  | Added on Thursday, December 31, 2015, 08:29 PM

I am here to propose to you today that we need to balance the risks and opportunities of advanced Artificial Intelligence. We should avoid the risks and, insofar as it is possible, realize the opportunities. We should not needlessly confront entirely unnecessary dangers. To achieve these goals, we must plan wisely and rationally. We should not act in fear and panic, or give in to technophobia; but neither should we act in blind enthusiasm. We should respect the interests of all parties with a stake in the Singularity. We must try to ensure that the benefits of advanced technologies accrue to as many individuals as possible, rather than being restricted to a few. We must try to avoid, as much as possible, violent conflicts using these technologies; and we must prevent massive destructive capability from falling into the hands of individuals. We should think through these issues before, not after, it is too late to do anything about them . 
==========
Rationality: From AI to Zombies (Eliezer Yudkowsky)
- Highlight Loc. 1657-63  | Added on Thursday, December 31, 2015, 08:30 PM

What are you to do? You certainly can’t use “probabilities.” We all know from school that “probabilities” are little numbers that appear next to a word problem, and there aren’t any little numbers here. Worse, you feel uncertain. You don’t remember feeling uncertain while you were manipulating the little numbers in word problems. College classes teaching math are nice clean places, therefore math itself can’t apply to life situations that aren’t nice and clean. You wouldn’t want to inappropriately transfer thinking skills from one context to another . Clearly, this is not a matter for “probabilities.” 
==========
Rationality: From AI to Zombies (Eliezer Yudkowsky)
- Highlight Loc. 1711-12  | Added on Thursday, December 31, 2015, 08:33 PM

What is evidence? It is an event entangled, by links of cause and effect, with whatever you want to know about. 
==========
Rationality: From AI to Zombies (Eliezer Yudkowsky)
- Highlight Loc. 1726-27  | Added on Thursday, December 31, 2015, 08:34 PM

This is why rationalists put such a heavy premium on the paradoxical-seeming claim that a belief is only really worthwhile if you could, in principle, be persuaded to believe otherwise. 
==========
Rationality: From AI to Zombies (Eliezer Yudkowsky)
- Highlight Loc. 1727-30  | Added on Thursday, December 31, 2015, 08:35 PM

If your retina ended up in the same state regardless of what light entered it, you would be blind. Some belief systems, in a rather obvious trick to reinforce themselves, say that certain beliefs are only really worthwhile if you believe them unconditionally—no matter what you see, no matter what you think. Your brain is supposed to end up in the same state regardless. Hence the phrase, “blind faith.” If what you believe doesn’t depend on what you see, you’ve been blinded as effectively as by poking out your eyeballs. 
==========
Rationality: From AI to Zombies (Eliezer Yudkowsky)
- Highlight Loc. 1731-32  | Added on Thursday, December 31, 2015, 08:35 PM

If your eyes and brain work correctly, your beliefs will end up entangled with the facts. Rational thought produces beliefs which are themselves evidence. 
==========
Rationality: From AI to Zombies (Eliezer Yudkowsky)
- Highlight Loc. 1735-38  | Added on Thursday, December 31, 2015, 08:35 PM

Therefore rational beliefs are contagious, among honest folk who believe each other to be honest. And it’s why a claim that your beliefs are not contagious—that you believe for private reasons which are not transmissible—is so suspicious. If your beliefs are entangled with reality, they should be contagious among honest folk. 
==========
Rationality: From AI to Zombies (Eliezer Yudkowsky)
- Highlight Loc. 1769-72  | Added on Thursday, December 31, 2015, 08:39 PM

Like a court system, science as a social process is made up of fallible humans. We want a protected pool of beliefs that are especially reliable. And we want social rules that encourage the generation of such knowledge. So we impose special, strong, additional standards before we canonize rational knowledge as “scientific knowledge,” adding it to the protected belief pool. 
==========
Rationality: From AI to Zombies (Eliezer Yudkowsky)
- Highlight Loc. 1863-64  | Added on Thursday, December 31, 2015, 08:45 PM

I try to avoid criticizing people when they are right. If they genuinely deserve criticism, I will not need to wait long for an occasion where they are wrong. 
==========
Rationality: From AI to Zombies (Eliezer Yudkowsky)
- Highlight Loc. 1882-83  | Added on Thursday, December 31, 2015, 08:46 PM

Hunches can be mysterious to the huncher, but they can’t violate the laws of physics. 
==========
Rationality: From AI to Zombies (Eliezer Yudkowsky)
- Highlight Loc. 1909-10  | Added on Thursday, December 31, 2015, 08:47 PM

“Witch,” itself, is a label for some extraordinary assertions—just because we all know what it means doesn’t mean the concept is simple. 
==========
Rationality: From AI to Zombies (Eliezer Yudkowsky)
- Highlight Loc. 1961-62  | Added on Thursday, December 31, 2015, 08:51 PM

The real sneakiness was concealed in the word “it” of “A witch did it.” A witch did what? 
==========
Rationality: From AI to Zombies (Eliezer Yudkowsky)
- Highlight Loc. 2220-21  | Added on Wednesday, January 06, 2016, 02:48 PM

In the school system, it’s all about verbal behavior, whether written on paper or spoken aloud. Verbal behavior gets you a gold star or a failing grade. Part of unlearning this bad habit is becoming consciously aware of the difference between an explanation and a password. 
==========
Rationality: From AI to Zombies (Eliezer Yudkowsky)
- Highlight Loc. 2233-34  | Added on Wednesday, January 06, 2016, 02:49 PM

I notice that I am confused . 
==========
Rationality: From AI to Zombies (Eliezer Yudkowsky)
- Highlight Loc. 2265-68  | Added on Wednesday, January 06, 2016, 02:51 PM

I encounter people who are quite willing to entertain the notion of dumber-than-human Artificial Intelligence, or even mildly smarter-than-human Artificial Intelligence. Introduce the notion of strongly superhuman Artificial Intelligence, and they’ll suddenly decide it’s “pseudoscience.” 
==========
Rationality: From AI to Zombies (Eliezer Yudkowsky)
- Highlight Loc. 2268-70  | Added on Wednesday, January 06, 2016, 02:51 PM

It’s not that they think they have a theory of intelligence which lets them calculate a theoretical upper bound on the power of an optimization process. Rather, they associate strongly superhuman AI to the literary genre of apocalyptic literature; whereas an AI running a small corporation associates to the literary genre of Wired magazine. 
==========
Rationality: From AI to Zombies (Eliezer Yudkowsky)
- Highlight Loc. 2270-71  | Added on Wednesday, January 06, 2016, 02:51 PM

They aren’t speaking from within a model of cognition. They don’t realize they need a model. They don’t realize that science is about models. 
==========
Rationality: From AI to Zombies (Eliezer Yudkowsky)
- Highlight Loc. 2274-75  | Added on Wednesday, January 06, 2016, 02:52 PM

Is there any idea in science that you are proud of believing, though you do not use the belief professionally? 
==========
Rationality: From AI to Zombies (Eliezer Yudkowsky)
- Highlight Loc. 2291-92  | Added on Wednesday, January 06, 2016, 02:56 PM

This was an earlier age of science. For a long time, no one realized there was a problem. Fake explanations don’t feel fake. That’s what makes them dangerous. 
==========
Rationality: From AI to Zombies (Eliezer Yudkowsky)
- Highlight Loc. 2344-45  | Added on Wednesday, January 06, 2016, 03:01 PM

Speaking of “hindsight bias” is just the nontechnical way of saying that humans do not rigorously separate forward and backward messages, allowing forward messages to be contaminated by backward ones. 
==========
Rationality: From AI to Zombies (Eliezer Yudkowsky)
- Highlight Loc. 2379-82  | Added on Wednesday, January 06, 2016, 03:05 PM

Jonathan Wallace suggested that “God!” functions as a semantic stopsign —that it isn’t a propositional assertion, so much as a cognitive traffic signal: do not think past this point. 
==========
Rationality: From AI to Zombies (Eliezer Yudkowsky)
- Highlight Loc. 2397-2400  | Added on Wednesday, January 06, 2016, 03:06 PM

Or suppose that someone says “Mexican-Americans are plotting to remove all the oxygen in Earth’s atmosphere.” You’d probably ask, “Why would they do that? Don’t Mexican-Americans have to breathe too? Do Mexican-Americans even function as a unified conspiracy?” If you don’t ask these obvious next questions when someone says, “Corporations are plotting to remove Earth’s oxygen,” then “Corporations!” functions for you as a semantic stopsign. 
==========
Rationality: From AI to Zombies (Eliezer Yudkowsky)
- Highlight Loc. 2512  | Added on Wednesday, January 06, 2016, 03:20 PM

the junk food of curiosity. 
==========
Rationality: From AI to Zombies (Eliezer Yudkowsky)
- Highlight Loc. 2570-71  | Added on Wednesday, January 06, 2016, 03:26 PM

Marcello and I developed a convention in our AI work: when we ran into something we didn’t understand, which was often, we would say “magic”—as in, “X magically does Y”—to remind ourselves that here was an unsolved problem, a gap in our understanding. 
==========
Rationality: From AI to Zombies (Eliezer Yudkowsky)
- Highlight Loc. 2610-13  | Added on Wednesday, January 06, 2016, 03:29 PM

I have been writing for quite some time now on the notion that the strength of a hypothesis is what it can’t explain, not what it can —if you are equally good at explaining any outcome, you have zero knowledge. So to spot an explanation that isn’t helpful, it’s not enough to think of what it does explain very well—you also have to search for results it couldn’t explain, 
==========
Rationality: From AI to Zombies (Eliezer Yudkowsky)
- Highlight Loc. 2732-35  | Added on Sunday, January 10, 2016, 11:55 PM

The way Traditional Rationality is designed, it would have been acceptable for me to spend thirty years on my silly idea, so long as I succeeded in falsifying it eventually, and was honest with myself about what my theory predicted, and accepted the disproof when it arrived, et cetera. This is enough to let the Ratchet of Science click forward, but it’s a little harsh on the people who waste thirty years of their lives. 
==========
Rationality: From AI to Zombies (Eliezer Yudkowsky)
- Highlight Loc. 2784-86  | Added on Sunday, January 10, 2016, 11:59 PM

In our ancestral environment, there were no movies; what you saw with your own eyes was true. Is it any wonder that fictions we see in lifelike moving pictures have too great an impact on us? Conversely, things that really happened, we encounter as ink on paper; they happened, but we never saw them happen. We don’t remember them happening to us. 
==========
Rationality: From AI to Zombies (Eliezer Yudkowsky)
- Highlight Loc. 2786-87  | Added on Sunday, January 10, 2016, 11:59 PM

The inverse error is to treat history as mere story, process it with the same part of your mind that handles the novels you read. 
==========
Rationality: From AI to Zombies (Eliezer Yudkowsky)
- Highlight Loc. 2796-98  | Added on Monday, January 11, 2016, 12:01 AM

I also realized that if I had actually experienced the past—if I had lived through past scientific revolutions myself, rather than reading about them in history books—I probably would not have made the same mistake again. I would not have come up with another mysterious answer; the first thousand lessons would have hammered home the moral. 
==========
Rationality: From AI to Zombies (Eliezer Yudkowsky)
- Highlight Loc. 2802-5  | Added on Monday, January 11, 2016, 12:01 AM

Why should I remember the Wright Brothers’ first flight? I was not there. But as a rationalist, could I dare to not remember, when the event actually happened? Is there so much difference between seeing an event through your eyes—which is actually a causal chain involving reflected photons, not a direct connection—and seeing an event through a history book? Photons and history books both descend by causal chains from the event itself. 
==========
Rationality: From AI to Zombies (Eliezer Yudkowsky)
- Highlight Loc. 2806  | Added on Monday, January 11, 2016, 12:01 AM

I had to overcome the false amnesia of being born at a particular time. 
==========
Rationality: From AI to Zombies (Eliezer Yudkowsky)
- Highlight Loc. 2902-4  | Added on Monday, January 11, 2016, 12:08 AM

Do you know how your knees work? Do you know how your shoes were made? Do you know why your computer monitor glows? Do you know why water is wet? The world around you is full of puzzles. 
==========
Rationality: From AI to Zombies (Eliezer Yudkowsky)
- Highlight Loc. 2961-62  | Added on Monday, January 11, 2016, 12:53 AM

That which you cannot make yourself, you cannot remake when the situation calls for it. 
==========
Rationality: From AI to Zombies (Eliezer Yudkowsky)
- Highlight Loc. 2967-69  | Added on Monday, January 11, 2016, 12:54 AM

When you contain the source of a thought, that thought can change along with you as you acquire new knowledge and new skills. When you contain the source of a thought, it becomes truly a part of you and grows along with you. 
==========
Rationality: From AI to Zombies (Eliezer Yudkowsky)
- Highlight Loc. 2969  | Added on Monday, January 11, 2016, 12:54 AM

Strive to make yourself the source of every thought worth thinking. 
==========
Rationality: From AI to Zombies (Eliezer Yudkowsky)
- Highlight Loc. 2970-71  | Added on Monday, January 11, 2016, 12:54 AM

Continually ask yourself: “How would I regenerate the thought if it were deleted?” When you have an answer, imagine that knowledge being deleted as well. 
==========
Rationality: From AI to Zombies (Eliezer Yudkowsky)
- Highlight Loc. 3002-3  | Added on Monday, January 11, 2016, 12:56 AM

If confusion threatens when you interpret a metaphor as a metaphor, try taking everything completely literally. 
==========
Rationality: From AI to Zombies (Eliezer Yudkowsky)
- Highlight Loc. 3184  | Added on Monday, January 11, 2016, 01:11 AM

“I taught you everything you know, but I haven’t taught you everything I know,” I say. 
==========
Rationality: From AI to Zombies (Eliezer Yudkowsky)
- Highlight Loc. 3240-44  | Added on Monday, January 11, 2016, 01:17 AM

Mark frowns, puzzled. “That makes no sense. It doesn’t resolve the essential chicken-and-egg dilemma.” “Sure it does. The bucket method works whether or not you believe in it.” “That’s absurd!” sputters Mark. “I don’t believe in magic that works whether or not you believe in it!” “I said that too,” chimes in Autrey. “Apparently I was wrong.” 
==========
Rationality: From AI to Zombies (Eliezer Yudkowsky)
- Highlight Loc. 3257-59  | Added on Monday, January 11, 2016, 01:19 AM

“What’s this so-called ‘reality’ business? I understand what it means for a hypothesis to be elegant, or falsifiable, or compatible with the evidence. It sounds to me like calling a belief ‘true’ or ‘real’ or ‘actual’ is merely the difference between saying you believe something, and saying you really really believe something.” 
==========
Rationality: From AI to Zombies (Eliezer Yudkowsky)
- Highlight Loc. 3284  | Added on Monday, January 11, 2016, 01:23 AM

I award you 0.12 units of fitness. 
==========
Rationality: From AI to Zombies (Eliezer Yudkowsky)
- Highlight Loc. 3314-16  | Added on Monday, January 11, 2016, 01:26 AM

Mark smiles condescendingly. “Believe me, Autrey, you’re not the first person to think of such a simple question. There’s no point in presenting it to us as a triumphant refutation.” 
==========
Rationality: From AI to Zombies (Eliezer Yudkowsky)
- Highlight Loc. 3388-89  | Added on Monday, January 11, 2016, 01:28 AM

What should I believe? As it turns out, that question has a right answer. 
==========
Rationality: From AI to Zombies (Eliezer Yudkowsky)
- Highlight Loc. 3447-48  | Added on Monday, January 11, 2016, 01:34 AM

When something we care about is threatened—our world-view, our in-group, our social standing, or anything else—our thoughts and perceptions rally to their defense. 
==========
Rationality: From AI to Zombies (Eliezer Yudkowsky)
- Highlight Loc. 3520-21  | Added on Monday, January 11, 2016, 01:40 AM

You are not a Bayesian homunculus whose reasoning is “corrupted” by cognitive biases. You just are cognitive biases. 
==========
Rationality: From AI to Zombies (Eliezer Yudkowsky)
- Highlight Loc. 3668-69  | Added on Monday, January 11, 2016, 01:57 AM

Beware when you find yourself arguing that a policy is defensible rather than optimal; or that it has some benefit compared to the null action, rather than the best benefit of any action. 
==========
Rationality: From AI to Zombies (Eliezer Yudkowsky)
- Highlight Loc. 3682-85  | Added on Tuesday, February 09, 2016, 03:26 PM

It’s amazing how many Noble Liars and their ilk are eager to embrace ethical violations—with all due bewailing of their agonies of conscience—when they haven’t spent even five minutes by the clock looking for an alternative. There are some mental searches that we secretly wish would fail; and when the prospect of success is uncomfortable, people take the earliest possible excuse to give up. 
==========
Rationality: From AI to Zombies (Eliezer Yudkowsky)
- Highlight Loc. 3778-79  | Added on Tuesday, February 09, 2016, 03:35 PM

We can use words to describe numbers that small, but not feelings—a feeling that small doesn’t exist, 
==========
Rationality: From AI to Zombies (Eliezer Yudkowsky)
- Highlight Loc. 3919-20  | Added on Tuesday, February 23, 2016, 04:29 PM

It’s a difference of life-gestalt that isn’t easy to describe in words at all, let alone quickly. 
==========
Rationality: From AI to Zombies (Eliezer Yudkowsky)
- Highlight Loc. 4284-85  | Added on Sunday, March 27, 2016, 08:29 PM

Real tough-mindedness is saying, “Yes, sulfuric acid is a horrible painful death, and no, that mother of five children didn’t deserve it, but we’re going to keep the shops open anyway because we did this cost-benefit calculation.” 
==========
Rationality: From AI to Zombies (Eliezer Yudkowsky)
- Highlight Loc. 4285-87  | Added on Sunday, March 27, 2016, 08:29 PM

Can you imagine a politician saying that? Neither can I. But insofar as economists have the power to influence policy, it might help if they could think it privately—maybe even say it in journal articles, suitably dressed up in polysyllabismic obfuscationalization so the media can’t quote it. 
==========
Rationality: From AI to Zombies (Eliezer Yudkowsky)
- Highlight Loc. 4348-49  | Added on Sunday, March 27, 2016, 08:33 PM

To suggest otherwise is to shoulder a burden of improbability. 
==========
Rationality: From AI to Zombies (Eliezer Yudkowsky)
- Highlight Loc. 4468-70  | Added on Sunday, March 27, 2016, 08:43 PM

Someone once said, “Not all conservatives are stupid, but most stupid people are conservatives.” If you cannot place yourself in a state of mind where this statement, true or false, seems completely irrelevant as a critique of conservatism, you are not ready to think rationally about politics. 
==========
Rationality: From AI to Zombies (Eliezer Yudkowsky)
- Highlight Loc. 4473-76  | Added on Sunday, March 27, 2016, 08:43 PM

In Hansonian terms: Your instinctive willingness to believe something will change along with your willingness to affiliate with people who are known for believing it—quite apart from whether the belief is actually true. Some people may be reluctant to believe that God does not exist, not because there is evidence that God does exist, but rather because they are reluctant to affiliate with Richard Dawkins or those darned “strident” atheists who go around publicly saying “God does not exist.” 
==========
Axiomatic (Greg Egan)
- Highlight on Page 7 | Loc. 108  | Added on Friday, April 15, 2016, 07:14 PM

The dreamers must flow. 
==========
Axiomatic (Greg Egan)
- Highlight on Page 12 | Loc. 175-79  | Added on Friday, April 15, 2016, 07:19 PM

If I’m right, then of course it makes no difference what I do; if all the versions of me who received the tip-off simply marched out of the whirlpool, it would have no impact on the mission. A set of measure zero wouldn’t be missed. But my actions, as an individual, are always irrelevant in that sense; if I, and I alone, deserted, the loss would be infinitesimal. The catch is, I could never know that I was acting alone. 
==========
Axiomatic (Greg Egan)
- Highlight on Page 40 | Loc. 603-4  | Added on Monday, April 18, 2016, 07:59 AM

I’d rather swim in this cacophony of a million contradictory voices than drown in the smooth and plausible lies of those genocidal authors of history who control the Hazzard Machines.
==========
Axiomatic (Greg Egan)
- Highlight on Page 147 | Loc. 2252-53  | Added on Sunday, May 01, 2016, 11:32 AM

I couldn’t give up hoping that the next book I opened would start with the words, ‘One sunny morning a boy woke up, and wondered what his name was.’ 
==========
Axiomatic (Greg Egan)
- Highlight on Page 149 | Loc. 2279-82  | Added on Sunday, May 01, 2016, 11:35 AM

For the past eleven years now, I’ve been spending my days at the host’s workplace. It’s certainly not for the host’s sake; I’m far more likely to get him sacked by screwing up at his job than by causing him one day’s absence every three years. It’s, well, it’s what I do, it’s who I am these days. Everybody has to define themselves somehow; I am a professional impersonator. The pay and conditions are variable, but a vocation cannot be denied.
==========
Permutation City (Greg Egan)
- Highlight on Page 4 | Loc. 50-51  | Added on Thursday, May 05, 2016, 11:53 AM

The fact that his view of the room remained flawless only made it worse, an irrefutable paranoid fixation: No matter how fast you turn your head, you’ll never even catch a glimpse of what’s going on all around you…
==========
Structure and Interpretation of Computer Programs, 2nd Edition (Harold Autor Abelson, Gerald Jay Autor Sussman and Julie Autor Sussman)
- Highlight on Page 36 | Loc. 552-53  | Added on Saturday, May 14, 2016, 06:23 PM

In general, when modeling phenomena in science and engineering, we begin with simplified, incomplete models. As we examine things in greater detail, these simple models become inadequate and must be replaced by more refined models.
==========
The Age of Em: Work, Love and Life when Robots Rule the Earth (Robin Hanson)
- Highlight Loc. 831-33  | Added on Monday, June 06, 2016, 04:42 PM

Of course by not considering any particular regulations, I can also be accused of being biased against regulation. When it comes to avoiding accusations of political bias, there is no absolutely safe ground in social science. 
==========
The Age of Em: Work, Love and Life when Robots Rule the Earth (Robin Hanson)
- Highlight Loc. 927-33  | Added on Monday, June 06, 2016, 04:54 PM

This book violates a standard taboo, in that it assumes that our social systems will mostly fail to prevent outcomes that many find lamentable, such as robots dominating the world, sidelining ordinary humans, and eliminating human abilities to earn wages. Once we have framed a topic as a problem that we’d want our social systems to solve, it is taboo to discuss the consequences of a failure to solve that problem. Discussing such consequences is usually only acceptable as a way to scare people into trying harder to solve the problem. Instead, analyzing in detail the consequences of failure, to learn how to live with such failure, is widely seen as expressing disloyalty to your social systems and hostility toward those who would suffer from its failure. 
==========
The Age of Em: Work, Love and Life when Robots Rule the Earth (Robin Hanson)
- Highlight Loc. 7026-27  | Added on Monday, June 06, 2016, 06:45 PM

Jordan, Gabriele, Samir Deeb, Jenny Bosten, and J. D. Mollon. 2010. “The Dimensionality of Color Vision in Carriers of Anomalous Trichromacy.” Journal of Vision 10(8): 12. 
==========
The Age of Em: Work, Love and Life when Robots Rule the Earth (Robin Hanson)
- Highlight Loc. 2158-61  | Added on Sunday, June 12, 2016, 01:52 PM

As the loyalty and reliability of an em is especially important in unusual crisis situations, simulations designed to test loyalties disproportionately portray such situations. Thus an em who finds itself in what seems to be an unusual crisis should suspect that it is in a simulation designed to test its loyalty and ability. 
==========
The Age of Em: Work, Love and Life when Robots Rule the Earth (Robin Hanson)
- Highlight Loc. 2392-93  | Added on Sunday, June 12, 2016, 03:30 PM

Retired ems are also good for unspecialized everyman roles, such as juror or voter, if they aren’t running too slowly to sufficiently understand the current society. 
==========
The Age of Em: Work, Love and Life when Robots Rule the Earth (Robin Hanson)
- Highlight Loc. 2417-18  | Added on Sunday, June 12, 2016, 03:34 PM

Ghosts are anti-social, avoid groups of more than a few humans, don’t seem to collect into ghost gangs 
==========
The Age of Em: Work, Love and Life when Robots Rule the Earth (Robin Hanson)
- Highlight Loc. 2426-27  | Added on Sunday, June 12, 2016, 03:34 PM

Such ghosts are real, and with trouble one can talk to them, but they aren’t very useful as allies, they get less moral weight, and one can usually ignore them without much cost. Because ems must pay 
==========
The Age of Em: Work, Love and Life when Robots Rule the Earth (Robin Hanson)
- Highlight Loc. 2426-27  | Added on Sunday, June 12, 2016, 03:35 PM

Such ghosts are real, and with trouble one can talk to them, but they aren’t very useful as allies, they get less moral weight, and one can usually ignore them without much cost. 
==========
The Age of Em: Work, Love and Life when Robots Rule the Earth (Robin Hanson)
- Highlight Loc. 2438-39  | Added on Sunday, June 12, 2016, 03:36 PM

Human aversion to death has many causes, but surely a big one is that growing productive humans takes years. For us, death can be a very expensive loss; 
==========
The Age of Em: Work, Love and Life when Robots Rule the Earth (Robin Hanson)
- Highlight Loc. 2439  | Added on Sunday, June 12, 2016, 03:36 PM

we often most mourn the deaths of young adults, in whom we have invested the most yet gained the least. 
==========
The Age of Em: Work, Love and Life when Robots Rule the Earth (Robin Hanson)
- Highlight Loc. 2443  | Added on Sunday, June 12, 2016, 03:37 PM

When life is cheap, death can be cheap as well. 
==========
The Age of Em: Work, Love and Life when Robots Rule the Earth (Robin Hanson)
- Highlight Loc. 2483-84  | Added on Sunday, June 12, 2016, 03:44 PM

Individual em copies may come to see the choice to end a copy not as “Shall I end?” but instead as “Do I want to remember this?” 
==========
The Age of Em: Work, Love and Life when Robots Rule the Earth (Robin Hanson)
- Highlight Loc. 2518-19  | Added on Sunday, June 12, 2016, 03:48 PM

Today, we are aware that suicide can have large opportunity costs, 
==========
The Age of Em: Work, Love and Life when Robots Rule the Earth (Robin Hanson)
- Highlight Loc. 2523-24  | Added on Sunday, June 12, 2016, 03:49 PM

suicide switches could help ems respond to threats such as torture or rape. 
==========
The Age of Em: Work, Love and Life when Robots Rule the Earth (Robin Hanson)
- Highlight Loc. 2611-13  | Added on Sunday, June 12, 2016, 04:00 PM

History shows that slave owners are the most eager to create and own slaves when wages are high and rising. When wages are low it costs nearly as much to feed and house a slave as it does to hire a free worker ( Domar 1970 ). 
==========
